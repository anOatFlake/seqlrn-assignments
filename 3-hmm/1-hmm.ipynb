{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 3: Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) Isolated Word Recognition\n",
    "\n",
    "In this assignment, we'll be revising word recognition, this time using Hidden Markov Models (HMM).\n",
    "As with [assignment 1](https://github.com/seqlrn/assignments/tree/master/1-dynamic-programming), we'll be using the [free spoken digits](https://github.com/Jakobovski/free-spoken-digit-dataset) dataset.\n",
    "We will be using the [`pandas`](https://pandas.pydata.org/docs/) library for data handling and [`hmmlearn`](https://hmmlearn.readthedocs.io/en/latest/index.html) library for HMMs which depends on `numpy`.\n",
    "Install the `pandas` and `hmmlearn` packages in your working environment and get familiar with these modules.\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "Download Zohar Jackson's [free spoken digit](https://github.com/Jakobovski/free-spoken-digit-dataset) dataset.\n",
    "There's no need to clone, feel free to use a revision, like [v1.0.10](https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/tags/v1.0.10.tar.gz).\n",
    "The file naming convention is `{digitLabel}_{speakerName}_{index}.wav`.\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "As you can learn from the [tutorial](https://hmmlearn.readthedocs.io/en/latest/tutorial.html#), `hmmlearn` provides us with the base implementation of Hidden Markov Models; we'll be using the `hmm.GaussianHMM`, which implements HMMs with a single Gaussian emission probability per state.\n",
    "For a starter, build a basic isolated word recognizer that uses a separate model for each digit.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 To facilitate the selection of samples for speakers and digits, consider how you can store the data within a `pandas.DataFrame`.\n",
    "\n",
    "1.2 Compute the MFCC features for the complete data set (3000 recordings; use `n_mfcc=13`).\n",
    "\n",
    "1.3 Apply per-speaker feature normalization (e.g., standardization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee275c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10#50 # recordings per speaker & digit\n",
    "DIGITS = [0, 1, 2, 3, 4]#, 5, 6, 7, 8, 9]\n",
    "SPEAKERS = [\"george\", \"jackson\"]#, \"lucas\", \"nicolas\", \"theo\", \"yweweler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9acf929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: a good default value is 25ms for FFT window and 10ms for hop length\n",
    "### Notice: be careful as librosa takes the number of samples as input!        \n",
    "\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def compute_features(file):\n",
    "    \"\"\"Computes the features for a recording file.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    y, sr = librosa.load(file)\n",
    "    n_mfcc = 13\n",
    "    hop = int(0.01 * sr)\n",
    "    nfft = int(0.025 * sr)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop, n_fft=nfft)\n",
    "\n",
    "    return mfccs.transpose()\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def load_dataframe(input_dir):\n",
    "    \"\"\"Loads the recordings into a pandas.DataFrame.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # df colums: speaker digit idx seq_len features\n",
    "    df_columns = ['speaker', 'digit', 'idx', 'seq_len', 'feature']\n",
    "    df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "    for speaker in SPEAKERS:\n",
    "        for digit in DIGITS:\n",
    "            for idx in range(0, NUM_SAMPLES):\n",
    "                filepath = input_dir + str(digit) + '_' + speaker + '_' + str(idx) + '.wav'\n",
    "                features = compute_features(filepath)\n",
    "                for i in range(0, len(features)) :\n",
    "                    df = pd.concat([df, pd.DataFrame([[speaker, digit, idx, i, features[i]]], columns=df_columns)], ignore_index=True)\n",
    "\n",
    "\n",
    "    return df       \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def normalize_features(dataframe):\n",
    "    \"\"\"Applies per-speaker feature normalization.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    df_norm = dataframe.copy()\n",
    "    for speaker in SPEAKERS:\n",
    "        for digit in DIGITS:\n",
    "            selector = (df_norm['speaker'] == speaker) & (df_norm['digit'] == digit)\n",
    "            features = df_norm[selector][\"feature\"]\n",
    "            flat_features = list(itertools.chain.from_iterable(features))\n",
    "\n",
    "            scaler = StandardScaler().fit([flat_features])\n",
    "            df_norm.loc[selector, 'feature'] = df_norm.loc[selector, 'feature'].apply(lambda x: scaler.transform([x]))\n",
    "            \n",
    "    return df_norm\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7466a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"C:/Dev/uni/seqlrn-assignments//3-hmm/data/recordings/\" \n",
    "dataframe = load_dataframe(input_dir=INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edcc5bb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 13 features, but StandardScaler is expecting 7592 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataframe_w_norm \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# TODO STANDARDISATION\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[36], line 58\u001b[0m, in \u001b[0;36mnormalize_features\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m     55\u001b[0m         flat_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(features))\n\u001b[0;32m     57\u001b[0m         scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\u001b[38;5;241m.\u001b[39mfit([flat_features])\n\u001b[1;32m---> 58\u001b[0m         df_norm\u001b[38;5;241m.\u001b[39mloc[selector, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_norm\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[36], line 58\u001b[0m, in \u001b[0;36mnormalize_features.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     55\u001b[0m         flat_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(features))\n\u001b[0;32m     57\u001b[0m         scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\u001b[38;5;241m.\u001b[39mfit([flat_features])\n\u001b[1;32m---> 58\u001b[0m         df_norm\u001b[38;5;241m.\u001b[39mloc[selector, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_norm\u001b[38;5;241m.\u001b[39mloc[selector, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_norm\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    301\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\preprocessing\\_data.py:1043\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1040\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1042\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1043\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 13 features, but StandardScaler is expecting 7592 features as input."
     ]
    }
   ],
   "source": [
    "dataframe_w_norm = normalize_features(dataframe=dataframe)\n",
    "# TODO STANDARDISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num recordings: 5046\n",
      "### george\n",
      "digit\n",
      "0    584\n",
      "1    549\n",
      "4    511\n",
      "3    465\n",
      "2    403\n",
      "Name: count, dtype: int64\n",
      "### jackson\n",
      "digit\n",
      "0    588\n",
      "1    534\n",
      "2    495\n",
      "3    484\n",
      "4    433\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Notice: just for test purposes\n",
    "\n",
    "print(\"Num recordings: {}\".format(len(dataframe)))\n",
    "for speaker in SPEAKERS:\n",
    "    print(\"### {}\".format(speaker))\n",
    "    data_speaker = dataframe[dataframe[\"speaker\"] == speaker]\n",
    "\n",
    "    print(data_speaker[\"digit\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e769c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num recordings: 5046\n",
      "### george\n",
      "digit\n",
      "0    584\n",
      "1    549\n",
      "4    511\n",
      "3    465\n",
      "2    403\n",
      "Name: count, dtype: int64\n",
      "### jackson\n",
      "digit\n",
      "0    588\n",
      "1    534\n",
      "2    495\n",
      "3    484\n",
      "4    433\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Num recordings: {}\".format(len(dataframe)))\n",
    "for speaker in SPEAKERS:\n",
    "    print(\"### {}\".format(speaker))\n",
    "    data_speaker = dataframe[dataframe[\"speaker\"] == speaker]\n",
    "\n",
    "    print(data_speaker[\"digit\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Evaluate\n",
    "\n",
    "2.1 Implement a 6-fold [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) loop for the 6 speakers to (later) figure out, which test speaker performs best/worst. That is, each speaker acts as test speaker while the others are used for training (with each possible combination).\n",
    "\n",
    "2.2 Inside the cross-validation loop, train an individual HMM with linear topology for each digit. There are several points to consider:\n",
    "\n",
    "*The [`fit`](https://github.com/hmmlearn/hmmlearn/blob/38b3cece4a6297e978a204099ae6a0a99555ec01/lib/hmmlearn/base.py#L439) expects features to be sequential in a single array with `X` as (n_train_samples, n_features). Furthermore, we need to pass the lengths of each recording into the function with`lengths` as (n_samples,):*\n",
    "\n",
    "```python\n",
    "### you can flatten the features of the train data as follows\n",
    "\n",
    "# input: [(rec_samples_1, n_feats), ..., (rec_samples_N, n_feats)]\n",
    "# output: (all_rec_samples, n_feats)\n",
    "features = [features for features in dataframe[\"features\"].values]\n",
    "flatten = np.concatenate(features, axis=0)\n",
    "\n",
    "lengths = np.array([...])\n",
    "\n",
    "# train HMM\n",
    "hmm.fit(X=flatten, lengths=lengths)\n",
    "```\n",
    "\n",
    "*For the HMM, it is necessary to choose a meaningful number of states. How many states (`n_components`) do you choose, and why?*\n",
    "\n",
    "*With respect to the used `hmmlearn` library. How can you enforce a linear topology?*\n",
    "\n",
    "*You might find that certain digits perform particularly bad; what could be a reason and how to mitigate it?*\n",
    "    \n",
    "2.3 Compute the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) for each speaker and for the overall dataset by combining the predictions of the cross-validation. You can use the [`scikit-learn`](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) library.\n",
    "\n",
    "2.4 Additional experiment: Compare the results without and with per-speaker feature normalization. How does the performance change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e903d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO:\n",
    "### 1. set the `n_components` for all digits (choose a meaningful number of states)\n",
    "\n",
    "n_comps = {i: None for i in DIGITS}\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Train\n",
    "for speaker in SPEAKERS:\n",
    "    \n",
    "\n",
    "# Evaluate\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: \n",
    "### 1. implement the 6-fold cross-validation loop\n",
    "### 2. allocate and initialize the HMMs, one for each digit; set a linear topology\n",
    "### 3. train the HMMs using the fit method; data needs to be concatenated\n",
    "### 4. evaluate the trained models on the test speaker; how do you decide which word\n",
    "###    was spoken?\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: \n",
    "### 1. based on the results, compute and display the confusion matrix for \n",
    "###    each test speaker \n",
    "### 2. compute and display the confusion matrix for the overall dataset\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278660a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2) Decoding Sequences of Digits\n",
    "\n",
    "The example above can't handle sequences of spoken digits.\n",
    "In this part of the assignment, you'll build a basic decoder that is able to decode arbitrary sequences of digits (without a prior, though).\n",
    "The `decode` method in `hmmlearn` only works for a single HMM.\n",
    "There are two ways how to solve this assignment:\n",
    "\n",
    "- Construct a \"meta\" HMM from the previously trained digit HMMs, by allowing state transitions from one digit to another; the resulting HMM can be decoded using the existing `decode` method (don't forget to re-map the state ids to the originating digit).\n",
    "\n",
    "- (Optional) Implement a real (time-synchronous) decoder using beam search. The straight-forward way is to maintain a (sorted) list of active hypotheses (ie. state history and current log-likelihood) that is first expanded and then pruned in each time step. The tricky part is at the \"end\" of a model: do you loop or expand new words?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Generate Test Sequences\n",
    "\n",
    "3.1 Generate a few test sequences of random length in between 3 and 6 digits; use [`numpy.random.randint`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randint.html) and be sure to also retain the digits sequence since we need to compute edit distance between reference and hypotheses later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b562cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digit_sequence(speaker_dataframe, min_digits, max_digits):\n",
    "    \"\"\"\n",
    "    Creates a sequence of spoken digits from a speaker and returns the\n",
    "    features and reference label.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eda3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice: just for test purposes\n",
    "\n",
    "# speaker = \"george\"\n",
    "# data_george = dataframe[dataframe[\"speaker\"] == speaker]\n",
    "# for i in range(20):\n",
    "#     data_seq, digits = create_digit_sequence(data_george)\n",
    "#     print(\"Digits: {}\".format(digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f453d4",
   "metadata": {},
   "source": [
    "### Create \"meta\" HMM\n",
    "\n",
    "4.1 Combine the previously trained HMMs to a single \"meta\" HMM, altering the transition probabilities to make a circular graph that allows each word to follow another.\n",
    "\n",
    "4.2 Implement a method that converts a state sequence relating to the meta HMM into a sequence of actual digits.\n",
    "\n",
    "4.3 Decode your test sequences and compute the [word error rate](https://en.wikipedia.org/wiki/Word_error_rate) (WER) with [JiWER](https://pypi.org/project/jiwer/) (install the package in your working environment).\n",
    "\n",
    "4.4 Compute an overall WER; ie. over the cross-validation.\n",
    "\n",
    "4.5 (Optional) Implement a basic time-synchronous beam search; how do the results compare to the above viterbi decoding in terms of accuracy and time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
