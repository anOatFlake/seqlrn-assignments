{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 2: POTUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) President of the United States (Trump vs. Obama)\n",
    "\n",
    "Surely, you're aware that the 45th President of the United States (@POTUS45) was an active user of Twitter, until (permanently) banned on Jan 8, 2021.\n",
    "You can still enjoy his greatness at the [Trump Twitter Archive](https://www.thetrumparchive.com/). We will be using original tweets only, so make sure to remove all retweets.\n",
    "Another fan of Twitter was Barack Obama (@POTUS43 and @POTUS44), who used the platform in a rather professional way.\n",
    "Please also consider the POTUS Tweets of Joe Biden; we will be using those for testing.\n",
    "\n",
    "### Data\n",
    "\n",
    "There are multiple ways to get the data, but the easiest way is to download the files from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group. \n",
    "Another way is to directly use the data from [Trump Twitter Archive](https://www.thetrumparchive.com/), [Obama Kaggle](https://www.kaggle.com/jayrav13/obama-white-house), and [Biden Kaggle](https://www.kaggle.com/rohanrao/joe-biden-tweets).\n",
    "Before you get started, please download the files; you can put them into the data folder.\n",
    "\n",
    "### N-gram Models\n",
    "\n",
    "In this assignment, you will be doing some Twitter-related preprocessing and training n-gram models to be able to distinguish between Tweets of Trump, Obama, and Biden.\n",
    "We will be using [NLTK](https://www.nltk.org), more specifically it's [`lm`](https://www.nltk.org/api/nltk.lm.html) module. \n",
    "Install the NLTK package within your working environment.\n",
    "You can use some of the NLTK functions, but you have to implement the functions for likelihoods and perplexity from scratch.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import nltk\n",
    "import pandas\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Prepare all the Tweets. Since the `lm` modules will work on tokenized data, implement a tokenization method that strips unnecessary tokens but retains special words such as mentions (@...) and hashtags (#...).\n",
    "\n",
    "1.2 Partition into training and test sets; select about 100 tweets each, which we will be testing on later. As with any Machine Learning task, training and test must not overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2ae5b5d-fccd-4092-af20-d8e8b4a65ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: ignore retweets \n",
    "\n",
    "def load_trump_tweets(filepath) -> list:\n",
    "    \"\"\"Loads all Trump tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    #     {\n",
    "    #     \"id\": 98454970654916600,\n",
    "    #     \"text\": \"Republicans and Democrats have both created our economic problems.\",\n",
    "    #     \"isRetweet\": \"f\",\n",
    "    #     \"isDeleted\": \"f\",\n",
    "    #     \"device\": \"TweetDeck\",\n",
    "    #     \"favorites\": 49,\n",
    "    #     \"retweets\": 255,\n",
    "    #     \"date\": \"2011-08-02 18:07:48\",\n",
    "    #     \"isFlagged\": \"f\"\n",
    "    #   },\n",
    "\n",
    "    df = pandas.read_json(filepath)\n",
    "    filtered_df = df[df['isRetweet'] == \"f\"]\n",
    "    \n",
    "    return(filtered_df['text'].to_list())\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def load_obama_tweets(filepath) -> list:\n",
    "    \"\"\"Loads all Obama tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Date,Username,Tweet-text,Tweet Link,Retweets,Likes,TweetImageUrl,Image\n",
    "    \n",
    "    with open(filepath, \"r\") as f:\n",
    "        df = pandas.read_csv(filepath)\n",
    "    return df['Tweet-text'].to_list()\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def load_biden_tweets(filepath) -> list:\n",
    "    \"\"\"Loads all Biden tweets and returns them as a list.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # id,timestamp,url,tweet,replies,retweets,quotes,likes\n",
    "\n",
    "    with open(filepath, \"r\") as f:\n",
    "        df = pandas.read_csv(filepath)\n",
    "    return df['tweet'].to_list()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f25e8c56-3837-440b-bebe-1916ebede6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: think about start and end tokens\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "NUM_TEST = 100\n",
    "\n",
    "def tokenize(text) -> list:\n",
    "    \"\"\"Tokenizes a single Tweet.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    tk = nltk.tokenize.TweetTokenizer()\n",
    "    tokens = tk.tokenize(text)\n",
    "    \n",
    "    return ['<s>'] + tokens + ['</s>']\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "\n",
    "def split_and_tokenize(data, num_test=NUM_TEST):\n",
    "    \"\"\"Splits and tokenizes the given list of Twitter tweets.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    tokenizedTweets: list = []\n",
    "    for tweet in data:\n",
    "        tokenizedTweets.append(tokenize(tweet))\n",
    "    \n",
    "    return tokenizedTweets\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2598cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = \"C:\\\\Dev\\\\uni\\\\seqlrn-assignments\\\\2-markov-chains\\\\data\\\\\"\n",
    "trump_tweets = split_and_tokenize(load_trump_tweets(basepath + \"tweets_01-08-2021.json\"))\n",
    "obama_tweets = split_and_tokenize(load_obama_tweets(basepath + \"Tweets-BarackObama.csv\"))\n",
    "biden_tweets = split_and_tokenize(load_biden_tweets(basepath + \"JoeBidenTweets.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c32c33-1a19-42ac-93d0-fcf66fbeaf5f",
   "metadata": {},
   "source": [
    "### Train N-gram Models\n",
    "\n",
    "2.1 Train n-gram models with n = [1, ..., 5] for Obama, Trump, and Biden.\n",
    "\n",
    "2.2 Also train a joint model, that will serve as background model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0828bda0-cef2-428b-a238-7f07f2c25425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_gram_models(n, data):\n",
    "    \"\"\"\n",
    "    To predict the first few words of the Tweet, we need the smaller n-grams as\n",
    "    well. This method does calculate all n-grams up to the given n.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ngrams = []\n",
    "    for i in range(1, n+1):\n",
    "        freq = nltk.FreqDist()\n",
    "        for tweet in data: \n",
    "            ngram = list(nltk.ngrams(tweet, i))\n",
    "            freq.update(ngram)\n",
    "             \n",
    "        ngrams.append(freq)\n",
    "    return ngrams\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "def get_suggestion(prev, n_gram_model):\n",
    "    \"\"\"\n",
    "    Gets the next random word for the given n_grams.\n",
    "    The size of the previous tokens must be exactly one less than the n-value\n",
    "    of the n-gram, or it will not be able to make a prediction.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    count = 1 #for laplace\n",
    "    possible_words = []\n",
    "    for ngram in n_gram_model.keys():\n",
    "        if list(ngram[:len(prev)]) == prev:\n",
    "            possible_words.append(ngram) \n",
    "            count += n_gram_model.get(ngram)\n",
    "\n",
    "    suggestions = []\n",
    "    for word in possible_words:\n",
    "        x = n_gram_model.get(word) / count  \n",
    "        suggestions.append((word[-1], x)) \n",
    "    suggestions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    selected = numpy.random.choice(len(suggestions))\n",
    "    return suggestions[selected][0]\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def get_random_tweet(n, n_gram_models):\n",
    "    \"\"\"Generates a random tweet using the given data set.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    tweet = ['<s>']\n",
    "    while tweet[-1] != '</s>':    \n",
    "        # use smaller ngram for start\n",
    "        if len(tweet) < n:\n",
    "            tweet.append(get_suggestion(tweet, n_gram_models[len(tweet)]))\n",
    "\n",
    "        # biggest ngram model for rest  \n",
    "        else:\n",
    "            tweet.append(get_suggestion(tweet[-n+1:], n_gram_models[n-1]))\n",
    "    \n",
    "    return tweet         \n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37e5cc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram models\n",
    "n_gram_models = {}\n",
    "n_gram_models['trump'] = build_n_gram_models(5, trump_tweets)\n",
    "n_gram_models['obama'] = build_n_gram_models(5, obama_tweets)\n",
    "n_gram_models['biden'] = build_n_gram_models(5, biden_tweets)\n",
    "n_gram_models['all'] = build_n_gram_models(5, trump_tweets + obama_tweets + biden_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6499c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Busy', 'doing', 'phoners', 'this', 'week', 'with', 'Neil', 'Cavuto', ',', '4', 'p', '.', 'm', '.', \"We'll\", 'be', 'discussing', 'current', 'affairs', 'and', 'politics', '.', 'Tune', 'in', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "random_tweet_trump = get_random_tweet(5, n_gram_models['trump'])\n",
    "print(random_tweet_trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648216bb-a49e-45ff-bb8c-9094c33acc07",
   "metadata": {},
   "source": [
    "### Classify the Tweets\n",
    "\n",
    "3.1 Use the log-ratio method to classify the Tweets for Trump vs. Biden. Trump should be easy to spot; but what about Obama vs. Biden?\n",
    "\n",
    "3.2 Analyze: At what context length (n) does the system perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "99dd4ca5-8094-40b0-aa1b-a51268659397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_single_token_log_ratio(prev, token, n_gram_model1, n_gram_model2):\n",
    "    \"\"\"Calculates the log ration of a token for two different n-grams\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    prob1 = n_gram_model1.freq(tuple(numpy.append(prev, token)))\n",
    "    prob2 = n_gram_model2.freq(tuple(numpy.append(prev, token)))\n",
    "    \n",
    "    return math.log((prob1 + 1e-10) / (prob2 + 1e-10))  # 1e-10 to avoid 0 division errors \n",
    "   \n",
    "    ### END YOUR CODE\n",
    "\n",
    "\n",
    "def classify(n, tokens, n_gram_models1, n_gram_models2):\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet.\n",
    "    If true is returned, the first one is more likely, otherwise the second.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    m1_log_ratio = 0\n",
    "    m2_log_ratio = 0\n",
    "\n",
    "    for i in range(0, len(tokens)):\n",
    "        m1_log_ratio += calculate_single_token_log_ratio(tokens[i-n:i], tokens[i], n_gram_models1[n-1], n_gram_models2[n-1])\n",
    "        m2_log_ratio = calculate_single_token_log_ratio(tokens[i-n:i], tokens[i], n_gram_models2[n-1], n_gram_models1[n-1])\n",
    "\n",
    "    return m1_log_ratio > m2_log_ratio\n",
    "    \n",
    "    ### END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "97d1e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(n, data1, data2, classify_fn):\n",
    "    \"\"\"\n",
    "    Trains the n-gram models on the train data and validates on the test data.\n",
    "    Uses the implemented classification function to predict the Tweeter.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    match = 0\n",
    "\n",
    "    # indexes where to split the data into train and test (80% to 20%)\n",
    "    testsplit_index_1 = int(len(data1) * 0.8)\n",
    "    testsplit_index_2 = int(len(data2) * 0.8)\n",
    "\n",
    "    d1_train = data1[0:testsplit_index_1]\n",
    "    d1_test = data1[testsplit_index_1:]\n",
    "    d2_train = data2[0:testsplit_index_2]\n",
    "    d2_test = data2[testsplit_index_2:]\n",
    "\n",
    "    d1_ngram_models = build_n_gram_models(n, d1_train)\n",
    "    d2_ngram_models = build_n_gram_models(n, d2_train)\n",
    "\n",
    "    for data in d1_test: \n",
    "        result = classify_fn(n, data, d1_ngram_models, d2_ngram_models)\n",
    "        if result == True: match += 1\n",
    "    for data in d2_test:\n",
    "        result = classify_fn(n, data, d1_ngram_models, d2_ngram_models)\n",
    "        if result == True: match += 1\n",
    "\n",
    "    value = match / (len(data1) + len(data2))\n",
    "    \n",
    "    print(\"score: {}\".format(value))\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "50a4ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_tweet = get_random_tweet(5, n_gram_models['trump'])\n",
    "obama_tweet = get_random_tweet(5, n_gram_models['obama'])\n",
    "biden_tweet = get_random_tweet(5, n_gram_models['biden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4f5f88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 1\n",
      "score: 0.20000758178854391\n",
      "score: 0.20007742934572204\n",
      "Context length: 2\n",
      "score: 0.0\n",
      "score: 0.0\n",
      "Context length: 3\n",
      "score: 0.0\n",
      "score: 0.0\n",
      "Context length: 4\n",
      "score: 0.0\n",
      "score: 0.0\n",
      "Context length: 5\n",
      "score: 0.0\n",
      "score: 0.0\n"
     ]
    }
   ],
   "source": [
    "context_length = 5\n",
    "for i in range(1, context_length+1):\n",
    "    print('Context length: {}'.format(i))\n",
    "    validate(i, trump_tweets, biden_tweets, classify_fn=classify)\n",
    "    validate(i, obama_tweets, biden_tweets, classify_fn=classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e1fb7-c67b-4e44-87c7-488e704c5ac1",
   "metadata": {},
   "source": [
    "### Compute Perplexities\n",
    "\n",
    "4.1 Compute (and plot) the perplexities for each of the test tweets and models. Is picking the Model with minimum perplexity a better classifier than in 3.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe2154-d816-442e-8de8-3b836ab0ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_perplexity(n, tokens, n_gram_models1, n_gram_models2):\n",
    "    \"\"\"\n",
    "    Checks which of the two given datasets is more likely for the given Tweet.\n",
    "    If true is returned, the first one is more likely, otherwise the second.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3be177",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 5\n",
    "for i in range(1, context_length+1):\n",
    "    validate(context_length, trump_tweets, biden_tweets, classify_fn=classify_with_perplexity)\n",
    "    validate(context_length, obama_tweets, biden_tweets, classify_fn=classify_with_perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
