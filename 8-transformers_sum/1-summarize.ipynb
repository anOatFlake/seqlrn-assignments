{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 8: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text-to-Text for Summarization\n",
    "\n",
    "Transformers are powerful attention-based models that have great power for generalization.\n",
    "Googles T5 is a popular and recent Text-to-Text transformer.\n",
    "A cool blog post about it can be found on Googles [AI.blog](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html).\n",
    "T5 is supposedly able to handle all kinds of NLP tasks like summarization, question answering, and text classification.\n",
    "The [paper](https://arxiv.org/abs/1910.10683) sheds some light on what they actually wanted to achieve.\n",
    "In our exercise we want to answer the question if we can use a pre-trained transformer model to summarize abstracts of our theses so they come close to their original titles?\n",
    "We will try this task in two ways: *out of the box* and *fine-tuning* on our dataset.\n",
    "Both should summarize the abstract to a thesis title.\n",
    "\n",
    "## Data\n",
    "\n",
    "Download and use the theses.csv dataset from the Supplemental Materials in the Files section of our Microsoft Teams group. \n",
    "This dataset contains approximately 3,000 thesis topics chosen by students in the past. Each entry includes a thesis title along with the corresponding abstract in the Abstract column. \n",
    "Note that not all entries include abstracts, so some filtering steps will be necessary.\n",
    "\n",
    "## Basic Setup\n",
    "\n",
    "First, you will need to download a German T5 model from this [link](https://huggingface.co/ml6team/mt5-small-german-finetune-mlsum)\n",
    "You can either clone the model repository or, preferably, download it directly using the [Transformers](https://huggingface.co/docs/transformers/index) library (recommended!).\n",
    "To get started, install the required libraries (you will need the sentencepiece tokenizer for this specific transformer model).\n",
    "Then, use the following code to load the tokenizer and model:\n",
    "```python\n",
    "# pip install transformers sentencepiece\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ml6team/mt5-small-german-finetune-mlsum\")\n",
    "```\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 As always... spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles.\n",
    "\n",
    "1.2 Remove very short titles, you can evaluate by a metric, e.g. len(abstract_words) / len(thesis title words). Furthermore, check the suitability of the model (e.g., max source / target lengths).\n",
    "\n",
    "1.3 Tokenize properly and implement helper classes / functions which handles your tokenized data with respect to your model and task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ef25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "    \n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Task 1: Out of the box\n",
    "\n",
    "Let's see if we can put this to use in our thesis example from the previous exercises.\n",
    "Follow the [instructions](https://huggingface.co/ozcangundes/mt5-small-turkish-summarization) for a Turkish summarization model\n",
    "on [Hugging Face](https://huggingface.co/). \n",
    "The steps should work the same for our German task.\n",
    "Implement some summarization and check qualitatively if you're content with the results.\n",
    "When generating, check some of the model parameters like **beamsize**, **repetition_penalty**, **length_penalty** and play around with them.\n",
    "**What do those parameters mean and how do they influence the output?**\n",
    "\n",
    "2.1 Set up the model and tokenizer with the Transformers library and download / load the pre-trained model weights of your choice!\n",
    "\n",
    "2.2 Implement a method for summary generation including preparation steps and configuration of model parameters. You need to consider `tokenizer.decode` to map the ids back to string tokens.\n",
    "\n",
    "2.3 Use the pre-trained model and your summary generation method to generate some thesis titles and (qualitatively) compare them to the reference titles. What generation parameters did you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "    \n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562cbb3c",
   "metadata": {},
   "source": [
    "### Task 2: Fine-tuning\n",
    "The big strength of those huge transformer model is that they can be fine-tuned on specific tasks. \n",
    "So lets try that for our ***abstract to thesis title*** summarization. \n",
    "Implement transfer learning of the model (e.g., mt5-small) you used in the previous task. \n",
    "and fine-tune the model on our dataset (input abstracts -> output titles).\n",
    "After fine-tuning try out the summarization and evaluate qualitatively again.\n",
    "Check the [documentation](https://huggingface.co/transformers/model_doc/t5.html) for what you need to do. \n",
    "This [notebook](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb) might also come in handy.\n",
    "\n",
    "3.1 As you could see, the title quality is pretty much hit-or-miss. Let's use transfer learning and fine-tune the pre-trained model to our title generation task. You can implement the training steps by yourself or use the [Trainer](https://huggingface.co/docs/transformers/training#trainer) of [Transformers](https://huggingface.co/docs/transformers/index).\n",
    "\n",
    "3.2 Fine-tune the pre-trained model on your dataset, you might need `T5ForConditionalGeneration` for the training steps. Subsequently, (qualitatively) evaluate the results on the same entries used in the previous task. Can you see any improvements compared to the *out of the box* summarization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08854933",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "    \n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
