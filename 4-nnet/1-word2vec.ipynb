{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2183f50-3d39-4832-af72-42791571713d",
   "metadata": {},
   "source": [
    "# Assignment 4: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996891f9-c12d-47bb-93f5-2f25cc60709b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1) Skip-grams\n",
    "\n",
    "Tomas Mikolov's [original paper](https://arxiv.org/abs/1301.3781) for word2vec is not very specific on how to actually compute the embedding matrices.\n",
    "Xin Ron provides a much more detailed [walk-through](https://arxiv.org/pdf/1411.2738.pdf) of the math, I recommend you go through it before you continue with this assignment.\n",
    "Now, while the original implementation was in C and estimates the matrices directly, in this assignment, we want to use PyTorch (and autograd) to train the matrices.\n",
    "There are plenty of example implementations and blog posts out there that show how to do it, I particularly recommend [Mateusz Bednarski's](https://towardsdatascience.com/implementing-word2vec-in-pytorch-skip-gram-model-e6bae040d2fb) version. Familiarize yourself with skip-grams and how to train them using pytorch.\n",
    "\n",
    "### Data\n",
    "\n",
    "Download the `theses.csv` data set from the `Supplemental Materials` in the `Files` section of our Microsoft Teams group.\n",
    "This dataset consists of approx. 3,000 theses topics chosen by students in the past.\n",
    "Here are some examples of the file content:\n",
    "\n",
    "```\n",
    "27.10.94;14.07.95;1995;intern;Diplom;DE;Monte Carlo-Simulation für ein gekoppeltes Round-Robin-System;\n",
    "04.11.94;14.03.95;1995;intern;Diplom;DE;Implementierung eines Testüberdeckungsgrad-Analysators für RAS;\n",
    "01.11.20;01.04.21;2021;intern;Bachelor;DE;Landessprachenerkennung mittels X-Vektoren und Meta-Klassifikation;\n",
    "```\n",
    "\n",
    "### Basic Setup\n",
    "\n",
    "For the upcoming assignments on Neural Networks, we'll be heavily using [PyTorch](https://pytorch.org) as go-to Deep Learning library.\n",
    "If you're not already familiar with PyTorch, now's the time to get started with it.\n",
    "Head over to the [Basics](https://pytorch.org/tutorials/beginner/basics/intro.html) and gain some understanding about the essentials.\n",
    "Before starting this assignment, make sure you've got PyTorch installed in your working environment. \n",
    "It's a quick setup, and you'll find all the instructions you need on the PyTorch website.\n",
    "As always, you can use [NumPy](https://numpy.org) and [Pandas](https://pandas.pydata.org) for data handling etc.\n",
    "\n",
    "*In this Jupyter Notebook, we will provide the steps to solve this task and give hints via functions & comments. However, code modifications (e.g., function naming, arguments) and implementation of additional helper functions & classes are allowed. The code aims to help you get started.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ffb0527-90ea-4f0a-ab0c-40817df51dfc",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/annika/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf666267-390d-402a-aae9-e3588b51c262",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1.1 Spend some time on preparing the dataset. It may be helpful to lower-case the data and to filter for German titles. The format of the CSV-file should be:\n",
    "\n",
    "```\n",
    "Anmeldedatum;Abgabedatum;JahrAkademisch;Art;Grad;Sprache;Titel;Abstract\n",
    "```\n",
    "\n",
    "1.2 Create the vocabulary from the prepared dataset. You'll need it for the initialization of the matrices and to map tokens to indices.\n",
    "\n",
    "1.3 Generate the training pairs with center word and context word. Which window size do you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c9e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_theses_dataset(filepath):\n",
    "    \"\"\"Loads all theses instances and returns them as a dataframe.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    df = pd.read_csv(filepath, sep=\";\")  \n",
    "    return df\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa699c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataframe):\n",
    "    \"\"\"Preprocesses and tokenizes the given theses titles for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    dataframe[\"Titel\"] = dataframe[\"Titel\"].apply(lambda x: x.replace(\"-\", \" \").replace(\"\\\"\", \"\").replace(\"(\", \"\").replace(\")\", \"\").lower())\n",
    "    dataframe['Titel'] = dataframe['Titel'].apply(lambda x: nltk.word_tokenize(x, language=\"german\"))\n",
    "    \n",
    "    return dataframe[\"Titel\"]\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3acde311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_pairs(data, word2idx, window_size):\n",
    "    \"\"\"Creates training pairs based on skip-grams for further use.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    idx_pairs = []\n",
    "    \n",
    "    for title in data:\n",
    "        indices = [word2idx[word] for word in title]\n",
    "        # for each word, threated as center word\n",
    "        for center_word_pos in range(len(indices)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                    continue\n",
    "                context_word_idx = indices[context_word_pos]\n",
    "                idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "    idx_pairs = np.array(idx_pairs)\n",
    "    \n",
    "    return idx_pairs\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "299d6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"D:/Dev/python/seqlrn-assignments/4-nnet/data/theses2022.csv\"\n",
    "path = \"4-nnet/data/theses2022.csv\"\n",
    "dataframe = load_theses_dataset(path)\n",
    "tokenized_data = preprocess(dataframe)\n",
    "\n",
    "vocabulary = []\n",
    "for title in tokenized_data.tolist():\n",
    "    for token in title:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "training_pairs = create_training_pairs(tokenized_data, word2idx, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb8bb6",
   "metadata": {},
   "source": [
    "### Train and Analyze\n",
    "\n",
    "2.1 Implement and train the word2vec model with your training data.\n",
    "\n",
    "2.2 Implement a method to find the top-k similar words for a given word (token).\n",
    "\n",
    "2.3 Analyze: What are the most similar words to \"Konzeption\", \"Cloud\" and \"virtuelle\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a65b62a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m     10\u001b[0m embedding_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m---> 11\u001b[0m W1 \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m(torch\u001b[38;5;241m.\u001b[39mrandn(embedding_dims, \u001b[38;5;28mlen\u001b[39m(vocabulary)\u001b[38;5;241m.\u001b[39mfloat(), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m     12\u001b[0m z1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(W1, x)\n\u001b[1;32m     14\u001b[0m W2 \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mrandn(vocabulary_size, embedding_dims)\u001b[38;5;241m.\u001b[39mfloat(), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "### TODO: 2.1 Implement and train the word2vec model.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Input layer\n",
    "def get_input_layer(word_idx):\n",
    "    x = torch.zeros(len(vocabulary)).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "# Hidden layer\n",
    "embedding_dims = 5\n",
    "W1 = Variable(torch.randn(embedding_dims, len(vocabulary).float(), requires_grad=True))\n",
    "z1 = torch.matmul(W1, x)\n",
    "\n",
    "# Output layer\n",
    "W2 = Variable(torch.randn(len(vocabulary), embedding_dims).float(), requires_grad=True)\n",
    "z2 = torch.matmul(W2, z1)\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02549c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.2 Implement a method to find the top-k similar words.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 2.3 Find the most similar words for \"Konzeption\", \"Cloud\" and \"virtuelle\".\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe48a9b",
   "metadata": {},
   "source": [
    "### Play with the Embeddings\n",
    "\n",
    "3.1 Use the computed embeddings: Can you identify the most similar theses for some examples?\n",
    "\n",
    "3.2 Visualize the embeddings for a subset of theses using [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). You can use [Scikit-Learn](https://scikit-learn.org/stable/) and [Matplotlib](https://matplotlib.org) or [Seaborn](https://seaborn.pydata.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.1 Compute the embeddings for the theses and transform with TSNE.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: 3.2 Visualize the samples in the 2D space.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "### END YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
